{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to pick up where we left off\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "- Finish text classification lesson by using stemming and lemmatization in our vectorizers\n",
    "- Build a simple text summarizer\n",
    "- How to find similar documents with cosine similarity and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenklee/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/darrenklee/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from time import time\n",
    "import pandas as pd\n",
    "pd.set_option(\"max.colwidth\", 500)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap our text classification section, we're going to learn how to incorporate stemming and lemmatization in our vectorizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\r\\n\\r\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ing...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\\r\\n\\r\\nIn any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We we...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I also dig their candy selection :)</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of paths, a desert xeriscape, baseball fields, ballparks, and a lake with ducks.\\r\\n\\r\\nThe Scottsdale Park and Rec Dept. does a wonderful job of keeping the park clean and shaded.  You can find trash cans and poopy-pick up mitts located all over the park and paths.\\r\\n\\r\\nThe fenced in area is huge to let the dogs run, play, and sniff!</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!! Not to go into detail, but let me assure you if you have any issues (albeit rare) speak with Scott and treat the guy with some respect as you state your case and I'd be surprised if you don't walk out totally satisfied as I just did. Like I always say..... \"Mistakes are inevitable, it's how we recover from them that is important\"!!!\\r\\n\\r\\nThanks to Scott and his awesome staff. You've got a customer for life!! .......... :^)</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0  My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\r\\n\\r\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ing...   \n",
       "1  I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\\r\\n\\r\\nIn any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We we...   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                         love the gyro plate. Rice is so good and I also dig their candy selection :)   \n",
       "3                                                                      Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of paths, a desert xeriscape, baseball fields, ballparks, and a lake with ducks.\\r\\n\\r\\nThe Scottsdale Park and Rec Dept. does a wonderful job of keeping the park clean and shaded.  You can find trash cans and poopy-pick up mitts located all over the park and paths.\\r\\n\\r\\nThe fenced in area is huge to let the dogs run, play, and sniff!   \n",
       "4                          General Manager Scott Petello is a good egg!!! Not to go into detail, but let me assure you if you have any issues (albeit rare) speak with Scott and treat the guy with some respect as you state your case and I'd be surprised if you don't walk out totally satisfied as I just did. Like I always say..... \"Mistakes are inevitable, it's how we recover from them that is important\"!!!\\r\\n\\r\\nThanks to Scott and his awesome staff. You've got a customer for life!! .......... :^)   \n",
       "\n",
       "     type                 user_id  cool  useful  funny  \n",
       "0  review  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  review  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  review  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  review  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  review  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in yelp review data\n",
    "\n",
    "path = \"../data/NLP_data/yelp.csv\"\n",
    "\n",
    "yelp = pd.read_csv(path, encoding='unicode-escape')\n",
    "\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame called yelp_best_worst that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    0.816691\n",
      "1    0.183309\n",
      "Name: stars, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "#Null accuracy\n",
    "print y.value_counts(normalize=True)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the analyzer section of the CountVectorizer doc strings\n",
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# analyzer = \"word\"... let's focus on this. Let's set it to a function instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyzer argument allows us to upload our function to transform/tokenize the words in our corpura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of stems\n",
    "def word_tokenize_stem(text):\n",
    "    words = TextBlob(text).words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "# define a function that accepts text and returns a list of lemons (noun version)\n",
    "\n",
    "def word_tokenize_lemma(text):\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]\n",
    "\n",
    "# define a function that accepts text and returns a list of lemons (verb version)\n",
    "def word_tokenize_lemma_verb(text):\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize(pos = \"v\") for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our three new functions with both count and tfidf vectorizers. \n",
    "<br>\n",
    "- First let's create a function that takes in an initialized but unfit vectorizer as an argument.\n",
    "- Fit and transforms training data using the vectorizer\n",
    "- Transforms the testing data\n",
    "- Fits naive bayes model on training data.\n",
    "- Evaluate it on the training and testing data.\n",
    "- Prints the number of features and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_model_evaluator(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    print \"Features: \", X_train_dtm.shape[1]\n",
    "    print \"Training Score: \", nb.score(X_train_dtm, y_train)\n",
    "    print \"Testing Score: \", nb.score(X_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  13273\n",
      "Training Score:  0.970626631854\n",
      "Testing Score:  0.924657534247\n"
     ]
    }
   ],
   "source": [
    "#Intialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_stem\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_stem)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  20599\n",
      "Training Score:  0.974216710183\n",
      "Testing Score:  0.904109589041\n"
     ]
    }
   ],
   "source": [
    "#Intialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  19431\n",
      "Training Score:  0.974216710183\n",
      "Testing Score:  0.906066536204\n"
     ]
    }
   ],
   "source": [
    "#Intialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma_verb\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma_verb)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret these results? Let's try it again with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stem is more accurate here, and more thorough. But there's still overfitting.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  13273\n",
      "Training Score:  0.816906005222\n",
      "Testing Score:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#Intialize Tfidf Vectorizer with stop_words set to english and analyzer to word_tokenize_stem\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=\"english\", analyzer=word_tokenize_stem)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  20599\n",
      "Training Score:  0.817232375979\n",
      "Testing Score:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#Intialize Tfidf Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma\n",
    "\n",
    "vect = TfidfVectorizer(stop_words= \"english\", analyzer=word_tokenize_lemma)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  19431\n",
      "Training Score:  0.817232375979\n",
      "Testing Score:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#Intialize Tfidf Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma\n",
    "\n",
    "vect = TfidfVectorizer(stop_words= \"english\", analyzer=word_tokenize_lemma_verb)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the tfidf vectorizers compare to counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The results are much worse, but it's not overfit : p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search time. Let's grid search objects that incorporate all of the analyzer functions for count and tfidf vectorizers. In addition we'll do the same for randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countvectorizer gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make pipeline for countvectorizer and naive bayes model\n",
    "pipe_cv = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "#Intialize parameters for count vectorizer\n",
    "param_grid_cv = {}\n",
    "param_grid_cv[\"countvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_cv[\"countvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_cv[\"countvectorizer__lowercase\"] = [True, False]\n",
    "param_grid_cv[\"countvectorizer__binary\"] = [True, False]\n",
    "param_grid_cv[\"countvectorizer__analyzer\"] = [\"word\", word_tokenize_stem,\n",
    "                                              word_tokenize_lemma, word_tokenize_lemma_verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Grid search object, this will run over 1000 models! \n",
    "\n",
    "grid_cv = GridSearchCV(pipe_cv, param_grid_cv, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#intialize time stamp\n",
    "t = time()\n",
    "#fit grid search object\n",
    "grid_cv.fit(X, y)\n",
    "#Print time elapsed\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Best parameters\n",
    "print grid_cv.best_params_\n",
    "#Best score\n",
    "print grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidfvectorizer gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make pipeline for tfidfvectorizer and naive bayes model\n",
    "pipe_tf = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "\n",
    "#Intialize parameters for tfidf vectorizer\n",
    "param_grid_tf = {}\n",
    "param_grid_tf[\"tfidfvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_tf[\"tfidfvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_tf[\"tfidfvectorizer__lowercase\"] = [True, False]\n",
    "param_grid_tf[\"tfidfvectorizer__binary\"] = [True, False]\n",
    "param_grid_tf[\"tfidfvectorizer__analyzer\"] = [\"word\", word_tokenize_stem,\n",
    "                                              word_tokenize_lemma, word_tokenize_lemma_verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Grid search object\n",
    "\n",
    "grid_tf = GridSearchCV(pipe_tf, param_grid_tf, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#intialize time stamp\n",
    "t = time()\n",
    "#fit grid search object\n",
    "grid_tf.fit(X, y)\n",
    "#Print time elapsed\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countvectorizer randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Randomized grid search with n_iter = 5\n",
    "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 5,\n",
    "                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "randsearch_cv.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Best params\n",
    "print randsearch_cv.best_params_\n",
    "#Best score\n",
    "print randsearch_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidfvectorizer randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Randomized grid search with n_iter = 10\n",
    "randsearch_tf = RandomizedSearchCV(pipe_tf, n_iter = 10,\n",
    "                        param_distributions = param_grid_tf, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "randsearch_tf.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Best params\n",
    "print randsearch_tf.best_params_\n",
    "#Best score\n",
    "print randsearch_tf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wraps up text classification. Now onto the rest of the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing text\n",
    "\n",
    "We're going to build a very simple summarizer that uses tfidf scores on a corpura of data science and artificial intelligence articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the greatest difficulties that companies wishing to become more analytical have encountered over the last several years is finding good analysts and data scientists. A considerable amount of printer’s ink has been spilled into articles over this issue. Many of them mention consultants’ or analyst firms’ projections about how many quantitative analysts or data scientists will be needed in our society and conclude that it will be incredibly difficult to find them.\\n\\nI always thought th...</td>\n",
       "      <td>What Data Scientist Shortage? Get Serious and Get Talent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Within soccer’s nascent analytics movement, one metric dominates most discussions. It’s called Expected Goals or xG. Models for calculating xG differ, but the underlying concept is the same. In a nutshell, xG takes a shot’s characteristics – distance from goal, angle from goal, root cause, etc. – and assigns a probability that said shot will result in a goal. Accounting for these probabilities reveals which team creates better scoring opportunities. Given a season of data, xG analysis is a p...</td>\n",
       "      <td>xG, Soccer Analytics of Bundesliga in R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The company’s adjacent market opportunities are growing at a CAGR of 18% for the next five years.\\n\\nQualcomm (NASDAQ: QCOM) announced a few days ago that its subsidiary Qualcomm Technologies will offer OEMs its first machine learning SDK for running their own neural network models on devices powered by Snapdragon 820 SoCs. The devices include smartphones, cars and drones among many others. Gary Brotman, director of product management, Qualcomm Technologies, said:\\n\\nWith the introduction of...</td>\n",
       "      <td>Qualcomm: Taking Artificial Intelligence To A New Level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Web, Tech Companies Use GPUs to Put Deep Learning at Your Fingertips\\n\\nGPUs have helped researchers spark a deep-learning revolution that’s given computers super-human capabilities.\\n\\nThey’ve already enabled breakthrough results on the industry-standard ImageNet benchmark. They’re powering Facebook’s “Big Sur” deep learning computing platform. They’re also accelerating major advances in deep learning across a broad range of fields.\\n\\nGPUs have become the go-to technology for training ...</td>\n",
       "      <td>How Companies Use GPUs to Put Deep Learning at Your Fingertips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White House technology policy adviser Kristen Honey urged government and industry IT leaders to support the open data movement and showcase their work at two upcoming data innovation events.\\n\\nSpeaking Wednesday to a standing-room-only audience at the annual Data Innovation Summit in Washington, Honey highlighted a number of the administration’s open data initiatives, dating back to 2009, that are leading to innovative advances in medicine, agriculture, energy, transportation and education....</td>\n",
       "      <td>White House official urges IT leaders to join open data efforts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0  One of the greatest difficulties that companies wishing to become more analytical have encountered over the last several years is finding good analysts and data scientists. A considerable amount of printer’s ink has been spilled into articles over this issue. Many of them mention consultants’ or analyst firms’ projections about how many quantitative analysts or data scientists will be needed in our society and conclude that it will be incredibly difficult to find them.\\n\\nI always thought th...   \n",
       "1  Within soccer’s nascent analytics movement, one metric dominates most discussions. It’s called Expected Goals or xG. Models for calculating xG differ, but the underlying concept is the same. In a nutshell, xG takes a shot’s characteristics – distance from goal, angle from goal, root cause, etc. – and assigns a probability that said shot will result in a goal. Accounting for these probabilities reveals which team creates better scoring opportunities. Given a season of data, xG analysis is a p...   \n",
       "2  The company’s adjacent market opportunities are growing at a CAGR of 18% for the next five years.\\n\\nQualcomm (NASDAQ: QCOM) announced a few days ago that its subsidiary Qualcomm Technologies will offer OEMs its first machine learning SDK for running their own neural network models on devices powered by Snapdragon 820 SoCs. The devices include smartphones, cars and drones among many others. Gary Brotman, director of product management, Qualcomm Technologies, said:\\n\\nWith the introduction of...   \n",
       "3  How Web, Tech Companies Use GPUs to Put Deep Learning at Your Fingertips\\n\\nGPUs have helped researchers spark a deep-learning revolution that’s given computers super-human capabilities.\\n\\nThey’ve already enabled breakthrough results on the industry-standard ImageNet benchmark. They’re powering Facebook’s “Big Sur” deep learning computing platform. They’re also accelerating major advances in deep learning across a broad range of fields.\\n\\nGPUs have become the go-to technology for training ...   \n",
       "4  White House technology policy adviser Kristen Honey urged government and industry IT leaders to support the open data movement and showcase their work at two upcoming data innovation events.\\n\\nSpeaking Wednesday to a standing-room-only audience at the annual Data Innovation Summit in Washington, Honey highlighted a number of the administration’s open data initiatives, dating back to 2009, that are leading to innovative advances in medicine, agriculture, energy, transportation and education....   \n",
       "\n",
       "                                                             title  \n",
       "0         What Data Scientist Shortage? Get Serious and Get Talent  \n",
       "1                          xG, Soccer Analytics of Bundesliga in R  \n",
       "2          Qualcomm: Taking Artificial Intelligence To A New Level  \n",
       "3   How Companies Use GPUs to Put Deep Learning at Your Fingertips  \n",
       "4  White House official urges IT leaders to join open data efforts  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in data\n",
    "\n",
    "path = \"../data/NLP_data/ds_articles.csv\"\n",
    "\n",
    "#We're only be using the text and title columns\n",
    "articles = pd.read_csv(path, usecols=[\"text\", \"title\"], encoding=\"utf-8\")\n",
    "\n",
    "#Drop nulls\n",
    "articles.dropna(inplace=True)\n",
    "\n",
    "#Reset index\n",
    "articles.reset_index(inplace=True, drop=True)\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1418, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1418 entries, 0 to 1417\n",
      "Data columns (total 2 columns):\n",
      "text     1418 non-null object\n",
      "title    1418 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 22.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Info\n",
    "articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf is a score of words based on how many times it showed up & ranks them w/ a higher value. \n",
    "# lower score are stock or useless words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Intialize tfidf with stop_words = english, max_features = 1000, and stem analzyer \n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=1000,\n",
    "                        analyzer=word_tokenize_stem)\n",
    "\n",
    "#Fit and transform the text using the tfidf vectorizer\n",
    "text = articles.text\n",
    "dtm = tfidf.fit_transform(text)\n",
    "\n",
    "#Assign tokens to features\n",
    "features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a dataframe of features and their idf scores\n",
    "idfscores = pd.DataFrame()\n",
    "idfscores[\"tokens\"] = features\n",
    "idfscores[\"scores\"] = tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>⭐️</td>\n",
       "      <td>7.564560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>var</td>\n",
       "      <td>6.311798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.772801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.961871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>blockchain</td>\n",
       "      <td>4.594146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>pdf</td>\n",
       "      <td>4.365887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>p</td>\n",
       "      <td>4.345685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>split</td>\n",
       "      <td>4.345685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>https</td>\n",
       "      <td>4.197265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>sql</td>\n",
       "      <td>4.146834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tokens    scores\n",
       "999          ⭐️  7.564560\n",
       "937         var  6.311798\n",
       "2           0.0  5.772801\n",
       "4           1.0  4.961871\n",
       "133  blockchain  4.594146\n",
       "639         pdf  4.365887\n",
       "625           p  4.345685\n",
       "822       split  4.345685\n",
       "422       https  4.197265\n",
       "824         sql  4.146834"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top ten most imporant words\n",
    "idfscores.sort_values(by = \"scores\", ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>to</td>\n",
       "      <td>1.012766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>the</td>\n",
       "      <td>1.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>of</td>\n",
       "      <td>1.020649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>a</td>\n",
       "      <td>1.024975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>and</td>\n",
       "      <td>1.025697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>in</td>\n",
       "      <td>1.033683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>is</td>\n",
       "      <td>1.049848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>for</td>\n",
       "      <td>1.049848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>that</td>\n",
       "      <td>1.067786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>with</td>\n",
       "      <td>1.070051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tokens    scores\n",
       "896     to  1.012766\n",
       "875    the  1.015625\n",
       "599     of  1.020649\n",
       "20       a  1.024975\n",
       "72     and  1.025697\n",
       "438     in  1.033683\n",
       "469     is  1.049848\n",
       "355    for  1.049848\n",
       "874   that  1.067786\n",
       "977   with  1.070051"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top ten least imporant words\n",
    "idfscores.sort_values(by = \"scores\", ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's our summarizer function that will randomly select an article to summarize. By summarize, I mean show the top five words with the highest tfidf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    \n",
    "    index = np.random.choice(articles.index, 1)[0]\n",
    "    article = text.iloc[index]\n",
    "    \n",
    "    word_scores = {}\n",
    "    for word in TextBlob(article).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[index, features.index(word)]\n",
    "    print \"TOP SCORING WORD: \"\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x:x[1], reverse = True)[:5]\n",
    "    \n",
    "    for word, score in top_scores:\n",
    "        print word\n",
    "        \n",
    "    print \"\\n\", articles.title[index]\n",
    "    \n",
    "    print \"\\n\\n\\n\", article \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORD: \n",
      "microsoft\n",
      "the\n",
      "that\n",
      "and\n",
      "app\n",
      "\n",
      "Silicon Valley’s Artificial Intelligence Marathon Is On\n",
      "\n",
      "\n",
      "\n",
      "Photo\n",
      "\n",
      "Artificial intelligence. Chatbots. Messaging. Sound familiar?\n",
      "\n",
      "These were some of the themes that Google brought up at its annual developer conference on Wednesday. At the event, the Silicon Valley company introduced an Internet-connected speaker called Google Home that is powered by A.I. and a new messaging app called Allo, among other things.\n",
      "\n",
      "These are also some of the very same topics that have come up at developer conferences held by Microsoft and Facebook this year. In March, Microsoft spent time talking about A.I. and bots, which are the pieces of software that can be used to produce new methods of interaction with computers, like chat interfaces. A month later, Facebook said it was opening up its Messenger messaging app so developers could create chatbots for the service.\n",
      "\n",
      "If there’s a certain sameness to it all, it illustrates how tech behemoths are all moving into an age of A.I. Microsoft, Google, Amazon and others are all racing to become the go-to company for A.I., betting that whoever wins will have the advantage in a technology that is permeating more and more software programs and hardware products.\n"
     ]
    }
   ],
   "source": [
    "#Give it a go\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity with Cosine Similarity and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "![ew](https://i2.wp.com/dataaspirant.com/wp-content/uploads/2015/04/cosine.png?w=697)\n",
    "<br><br>\n",
    "\" Cosine similarity metric finds the normalized dot product of the two attributes. By determining the cosine similarity, we would effectively try to find the cosine of the angle between the two objects. The cosine of 0° is 1, and it is less than 1 for any other angle.\n",
    "\n",
    "It is thus a judgement of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude.\n",
    "\n",
    "Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in (0,1). One of the reasons for the popularity of cosine similarity is that it is very efficient to evaluate, especially for sparse vectors.\"\n",
    "<br>\n",
    "Source: [Dataaspirant](http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972\n"
     ]
    }
   ],
   "source": [
    "#Diy cosine similarity function\n",
    "\n",
    "def square_rooted(x):\n",
    "\n",
    "    return round(np.sqrt(sum([a*a for a in x])),3)\n",
    " \n",
    "def cosine_similarity_function(x,y):\n",
    "\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return round(numerator/float(denominator),3)\n",
    " \n",
    "vec1 = [3, 45, 7, 2]\n",
    "vec2 = [2, 54, 13, 15]\n",
    "print cosine_similarity_function(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive matrix of similarities between all the data science articles documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate cosine distance for each pair of documents\n",
    "dist = cosine_similarity(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1418, 1418)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make it a dataframe\n",
    "dist_df = pd.DataFrame(dist)\n",
    "\n",
    "#Shape\n",
    "dist_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare some articles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Index position of article\n",
    "index = 239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Popular TV Shows on Data Science and Artificial Intelligence\n",
      "\n",
      " ************************************************ \n",
      "Introduction\n",
      "\n",
      "The development of full artificial intelligence could spell the end of human race. – Stephen Hawking\n",
      "\n",
      "The world is now rapidly moving towards achieving this finest technology breakthrough ever. It is expected that AI would enrich humans with more power and opportunities. Another group of people (including Stephen Hawking and Elon Musk) believe that this might lead to human destruction (if not handled carefully).\n",
      "\n",
      "I think, it’s too early for us to envisage such uncertain future. Good news is, companies like Google, Microsoft, Baidu have already started creating products based on AI. It won’t be long enough to experience the influence of AI in our daily lives.\n",
      "\n",
      "Accidentally, my exploration of AI started with movie ‘Her’. The influence was so powerful that I ended up creating an infographic on 10 Movies on Data Science and Machine Learning. May be a ~ 2 hours movie didn’t nourish my appetite well.\n",
      "\n",
      "Few days later, I came across a TV Series ‘Intelligence’. So far, this is one of the best show I’ve ever watched. Later, I found many other shows which beautifully describes a future world driven by data, technology, numbers and artificial intelligence. Here I’ve shared the best of them.\n",
      "\n",
      "Below are 10 Popular TV Shows on Data Science and Artificial Intelligence ( in no particular order). If you haven’t seen any of them, I’d suggest you to begin with ‘Intelligence’.\n",
      "\n",
      "If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.\n"
     ]
    }
   ],
   "source": [
    "#Assign titles column to titles variable\n",
    "\n",
    "titles = articles.title\n",
    "\n",
    "#Print title\n",
    "print titles[index]\n",
    "\n",
    "#print article\n",
    "print \"\\n ************************************************ \\n\" , text[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to take the index value and use it grab the column of the scores between every article and the one at index 935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pass index value into dataframe\n",
    "dist_column = dist_df[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the index values of the 5 \n",
    "\n",
    "closest_index = dist_column.nlargest(6).index[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why algorithms will be at the core of our AI-powered future, and why you should care\n",
      "Why We Need More Women Taking Part In The AI Revolution\n",
      "How AI Is Already Changing Business\n",
      "The Non-Technical Guide to Machine Learning & Artificial Intelligence\n",
      "Will a machine replace me?\n"
     ]
    }
   ],
   "source": [
    "#Pass index values into titles and print them\n",
    "\n",
    "for i in titles.iloc[closest_index].tolist():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130    For the second year in a row, We Are Social had the privilege of presenting at Vivid Sydney this year. Already one of the world’s leading festivals, Vivid is a bit like SXSW: an amalgam of inspiring people, creative work, and fresh ideas across a variety of themes and topics.\\n\\nFor our 2017 keynote, I joined forces with We Are Social’s Sydney MD, Suzie Shaw, to explore the impact that algorithms and machine learning are having on every aspect of our lives, and what that means for marketing....\n",
       "1000    Lolita Taub\\n\\nBy Samantha Walravens & Heather Cabot\\n\\nIn 2011, entrepreneur and investor Marc Andreessen wrote his famous ,\"Why Software Is Eating the World\" in the Wall Street Journal. Today, that story would more likely read, \"Why Artificial Intelligence Is Eating the World.\" The market for artificial intelligence (AI) technologies-- from voice and image recognition to chat bots to self-driving cars-- is hot. A Narrative Science survey found last year that 38% of enterprises are already ...\n",
       "1220    Erik Brynjolfsson, MIT Sloan School professor, explains how rapid advances in machine learning are presenting new opportunities for businesses. He breaks down how the technology works and what it can and can’t do (yet). He also discusses the potential impact of AI on the economy, how workforces will interact with it in the future, and suggests managers start experimenting now. Brynjolfsson is the co-author, with Andrew McAfee, of the HBR Big Idea article, “The Business of Artificial Intellig...\n",
       "1009    The Non-Technical Guide to Machine Learning & Artificial Intelligence\\n\\nI have a challenge for you.\\n\\nIn a few seconds, I want you to stop reading this article, and follow the instructions below.\\n\\nHere you go:\\n\\n2. Hit “crtl + f”\\n\\n3. Search “artificial intelligence” or “machine learning”\\n\\n4. If you don't find any matches, publicly shame me on Twitter.\\n\\nI hope my point is obvious.\\n\\nMachine learning and artificial intelligence (ML and AI) have seized Tech mindshare in a way few to...\n",
       "1076    A computer did not write this article.\\n\\nToday, I can say that and you’ll believe me. No mere machine could come up with such a killer opening. Only humans – very, very talented humans – can do such things.\\n\\nBut what about a reader 30 years from now? Would they buy it? Or would they point out that it’s exactly the sort of thing a computer would write, to try and sound more human? Or, perhaps, will the distinction be completely irrelevant by then?\\n\\nFor the benefit of any literal-minded n...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass index values into titles and but don't print\n",
    "text.iloc[closest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "It is standard practice to cluster with tfidf data instead of the count vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize clustering algorithm with 4 clusters and fit it on dtm\n",
    "km = KMeans(n_clusters= 4)\n",
    "\n",
    "#Fit algorithm\n",
    "km.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011012143631564825"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out silhouette score\n",
    "silhouette_score(dtm, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign labels to articles dataframe \n",
    "\n",
    "articles[\"cluster\"] = km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print 5 randomly selected headlines from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cheat Sheet] Python Basics For Data Science\n",
      "Phase Change Co-Processors Extend Memory Technologies To Computing\n",
      "Airbnb open sources data-science-sharing platform\n",
      "If you want to learn Data Science, take a few of these statistics classes\n",
      "Facebook’s Artificial Intelligence Research lab releases open source fastText on GitHub\n",
      "7 Visualizations You Should Learn in R\n",
      "HBase Key Design with OpenTSDB #WhiteboardWalkthrough\n",
      "How Data Scientist Skills and Qualifications Differ From Those of BI Analysts and Statisticians\n",
      "DeepMind just published a mind blowing paper: PathNet.\n",
      "Resources to Start Learning R Language – Beginner @ Data Science – Medium\n"
     ]
    }
   ],
   "source": [
    "#Cluster 0\n",
    "for i in articles[articles.cluster == 0].sample(n = 10).title.tolist():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What’s next for blockchain and cryptocurrency\n",
      "Machine Learning Is Redefining The Enterprise In 2016\n",
      "OracleVoice: Machine Learning Stands To Transform The Way We Communicate\n",
      "Heart Failure Identification Algorithms Developed Using EHR Data\n",
      "Samsung Will Invest $1.2 Billion Into US For 'Internet Of Things'\n",
      "3 Industries That Will Be Transformed By AI, Machine Learning And Big Data In The Next Decade\n",
      "Machine learning methods (infographic)\n",
      "Seattle is paying the most for its engineers\n",
      "Artificial Intelligence in the Next-Gen Automobile\n",
      "Artificial Intelligence Is Not [Only] All About Robots\n"
     ]
    }
   ],
   "source": [
    "#Cluster 1\n",
    "for i in articles[articles.cluster == 1].sample(n = 10).title.tolist():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Steps To Jumpstart A Machine Learning Strategy\n",
      "The age of analytics: Competing in a data-driven world\n",
      "Statistics and Machine Learning\n",
      "Cognitive Analytics Answers the Question: What's Interesting in Your Data?\n",
      "Internet Of Things (IoT): 5 Essential Ways Every Company Should Use It\n",
      "Are Your Predictive Models like Broken Clocks?\n",
      "The Skills You Need to Become a Data Scientist\n",
      "Hold Your Machine Learning and AI Models Accountable\n",
      "White House: Want data science with impact? Spend ‘a ridiculous amount of time’ with people\n",
      "5 Amazing Things Big Data Helps Us To Predict Now -- Plus What's On The Horizon\n"
     ]
    }
   ],
   "source": [
    "#Cluster 2\n",
    "for i in articles[articles.cluster == 2].sample(n = 10).title.tolist():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a deep learning model to steer a car in 99 lines of code\n",
      "This Super Bowl Experiment Proves Machine Learning Still Needs a Helping Hand From Humanity\n",
      "IBM Advances Neuromorphic Computing for Deep Learning\n",
      "Ten Myths About Machine Learning – Pedro Domingos – Medium\n",
      "How Artificial Intelligence Will Change Everything\n",
      "Artificial intelligence is about the people, not the machines\n",
      "Machine learning and AI: Can it be the butler that can change user experience?\n",
      "FLYR: Data Science Shaking Up Travel Industry\n",
      "Deep Learning Isn't a Dangerous Magic Genie. It's Just Math\n",
      "Answers to dozens of data science job interview questions\n"
     ]
    }
   ],
   "source": [
    "#Cluster 3\n",
    "for i in articles[articles.cluster == 3].sample(n = 10).title.tolist():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think the clusters are? Is it easy decipher? Ignore the silhouette score, does it pass the eye test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it's hard to interpret. Doesn't really pass the eye test.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this exercise again but this time we'll cluster the cosine distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize clustering algorithm with 4 clusters\n",
    "km2 = KMeans(n_clusters= 4)\n",
    "\n",
    "\n",
    "#fit it on dist array\n",
    "km2.fit(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28199394634494024"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out silhouette score\n",
    "silhouette_score(dist, km2.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print 5 randomly selected headlines from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assign new labels to data frame\n",
    "\n",
    "articles[\"cluster_dist\"] = km2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    506\n",
       "1    419\n",
       "0    255\n",
       "2    238\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM partnership puts Watson in your ear to help you at work\n",
      "Spark-based machine learning for capturing word meanings\n",
      "Five New Machine Learning Tools To Make Your Software Intelligent\n",
      "AI, IoT biggest disruptors in 2017\n",
      "IBM Invests in R Programming Language for Data Science; Joins R Consortium\n",
      "7 Steps to Mastering SQL for Data Science\n",
      "Thinking About Cloud Data Analytics? Consider This\n"
     ]
    }
   ],
   "source": [
    "#Cluster 0\n",
    "for i in articles[articles.cluster_dist == 0].sample(n = 7).title.tolist():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Future of Machine Learning in Finance\n",
      "Artificial intelligence, machine learning, deep learning and more\n",
      "7 More Steps to Mastering Machine Learning With Python\n",
      "A Technical Primer On Causality – adam kelleher – Medium\n",
      "Samsung Will Invest $1.2 Billion Into US For 'Internet Of Things'\n",
      "Predictive Analytics And Machine Learning AI In The Retail Supply Chain\n",
      "How Machine Learning, Big Data And AI Are Changing Healthcare Forever\n"
     ]
    }
   ],
   "source": [
    "#Cluster 1, Investment/Business related? #FORBESmagazine??\n",
    "for i in articles[articles.cluster_dist == 1].sample(n = 7).title.tolist():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning and big data know it wasn’t you who just swiped your credit card\n",
      "Salaries by Roles in Data Science and Business Intelligence\n",
      "Deep Learning - A Non-Technical Introduction\n",
      "A Complete Tutorial to learn Data Science in R from Scratch\n",
      "How to define two-dimensional array in python\n",
      "Deep Learning - A Non-Technical Introduction\n",
      "Machine learning in our daily lives\n"
     ]
    }
   ],
   "source": [
    "#Cluster 2, \n",
    "for i in articles[articles.cluster_dist == 2].sample(n = 7).title.tolist():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speech age\n",
      "Learning from Imbalanced Classes\n",
      "10 tips for getting started with machine learning\n",
      "Top 10 Big Data Challenges – A Serious Look at 10 Big Data V’s\n",
      "Three Things Your Organization Must Consider To Prepare For Artificial Intelligence\n",
      "From Big Data to Artificial Intelligence: The Next Digital Disruption\n",
      "Behind the Dream of Data Work as it Could Be\n"
     ]
    }
   ],
   "source": [
    "#Cluster 3, maybe more educational?\n",
    "for i in articles[articles.cluster_dist == 3].sample(n = 7).title.tolist():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "\n",
    "My fake news classifer article: https://opendatascience.com/blog/how-to-build-a-fake-news-classification-model/\n",
    "<br>\n",
    "My data science topic modeling article: https://opendatascience.com/blog/how-to-analyze-articles-about-data-science-using-data-science/\n",
    "<br><br>\n",
    "**Regular Expressions**\n",
    "- https://www.dataquest.io/blog/regular-expressions-data-scientists/\n",
    "- https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial\n",
    "- https://www.oreilly.com/ideas/an-introduction-to-regular-expressions\n",
    "\n",
    "\n",
    "**NLP Tutorials**\n",
    "\n",
    "- https://github.com/bonzanini/nlp-tutorial\n",
    "- https://github.com/totalgood/pycon-2016-nlp-tutorial\n",
    "\n",
    "**Text similarity:**\n",
    "- https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/\n",
    "- http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/\n",
    "- http://billchambers.me/tutorials/2014/12/22/cosine-similarity-explained-in-python.html\n",
    "- Explains why text similarity uses cosine similarity -> https://www.quora.com/What-are-the-mechanics-of-cosine-similarity-in-natural-language-processing\n",
    "\n",
    "**Text classification:**\n",
    "- Another fake news tutorial - > https://www.datacamp.com/community/tutorials/scikit-learn-fake-news\n",
    "- http://nlpforhackers.io/text-classification/\n",
    "- http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "- https://github.com/javedsha/text-classification\n",
    "- https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "- https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html\n",
    "\n",
    "\n",
    "**Text clustering:**\n",
    "\n",
    "- Great tutorial -> http://brandonrose.org/clustering\n",
    "- http://nlpforhackers.io/recipe-text-clustering/\n",
    "- https://pythonprogramminglanguage.com/kmeans-text-clustering/\n",
    "- http://mccormickml.com/2015/08/05/document-clustering-example-in-scikit-learn/\n",
    "\n",
    "\n",
    "**Word Embeddings/Word2Vec**\n",
    "\n",
    "- https://chatbotsmagazine.com/introduction-to-word-embeddings-55734fd7068a\n",
    "- https://www.springboard.com/blog/introduction-word-embeddings/\n",
    "- http://ruder.io/word-embeddings-1/\n",
    "- https://www.slideshare.net/BhaskarMitra3/a-simple-introduction-to-word-embeddings\n",
    "- https://github.com/fastai/word-embeddings-workshop\n",
    "\n",
    "\n",
    "**Topic Modeling**\n",
    "\n",
    "- http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "- https://blog.bigml.com/2016/11/16/introduction-to-topic-models/\n",
    "- http://nbviewer.jupyter.org/github/ogrisel/notebooks/blob/master/nmf_topics.ipynb?create=1\n",
    "- https://www.youtube.com/watch?v=ZgyA1Q2ywbM\n",
    "- https://www.youtube.com/watch?v=SjRss8Uk6mQ\n",
    "- https://github.com/derekgreene/topic-model-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab time\n",
    "\n",
    "Pick a text dataset to spend the rest of class working. There are three other datasets in the NLP_data that you can work with: pitchfork album reviews, fake/real news, deadspin, and political lean. Make sure to unzip political lean or fake news. You can also continue to work with the datasets we've already used (data science, yelp, spam.)\n",
    "\n",
    "<br>\n",
    "\n",
    "For the rest of class apply supervised or unsupervised learning techniques to the dataset of your choice. \n",
    "\n",
    "- Build a model that can differentiate between good/bad review, real/fake news, or liberal/conservative leaning or a model that \n",
    "\n",
    "- Predict how many page views a deadspin can get based on its headlines and tags.\n",
    "\n",
    "- Ignore the labels and attempt cluster the articles.\n",
    "\n",
    "- Have fun with the summarizer!!\n",
    "\n",
    "<br>\n",
    "\n",
    "Be prepared to share your results at the end of class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\r\\n\\r\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ing...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\\r\\n\\r\\nIn any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We we...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I also dig their candy selection :)</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of paths, a desert xeriscape, baseball fields, ballparks, and a lake with ducks.\\r\\n\\r\\nThe Scottsdale Park and Rec Dept. does a wonderful job of keeping the park clean and shaded.  You can find trash cans and poopy-pick up mitts located all over the park and paths.\\r\\n\\r\\nThe fenced in area is huge to let the dogs run, play, and sniff!</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!! Not to go into detail, but let me assure you if you have any issues (albeit rare) speak with Scott and treat the guy with some respect as you state your case and I'd be surprised if you don't walk out totally satisfied as I just did. Like I always say..... \"Mistakes are inevitable, it's how we recover from them that is important\"!!!\\r\\n\\r\\nThanks to Scott and his awesome staff. You've got a customer for life!! .......... :^)</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0  My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\r\\n\\r\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ing...   \n",
       "1  I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\\r\\n\\r\\nIn any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We we...   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                         love the gyro plate. Rice is so good and I also dig their candy selection :)   \n",
       "3                                                                      Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of paths, a desert xeriscape, baseball fields, ballparks, and a lake with ducks.\\r\\n\\r\\nThe Scottsdale Park and Rec Dept. does a wonderful job of keeping the park clean and shaded.  You can find trash cans and poopy-pick up mitts located all over the park and paths.\\r\\n\\r\\nThe fenced in area is huge to let the dogs run, play, and sniff!   \n",
       "4                          General Manager Scott Petello is a good egg!!! Not to go into detail, but let me assure you if you have any issues (albeit rare) speak with Scott and treat the guy with some respect as you state your case and I'd be surprised if you don't walk out totally satisfied as I just did. Like I always say..... \"Mistakes are inevitable, it's how we recover from them that is important\"!!!\\r\\n\\r\\nThanks to Scott and his awesome staff. You've got a customer for life!! .......... :^)   \n",
       "\n",
       "     type                 user_id  cool  useful  funny  \n",
       "0  review  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  review  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  review  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  review  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  review  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in yelp review data\n",
    "\n",
    "path = \"../data/NLP_data/yelp.csv\"\n",
    "\n",
    "yelp = pd.read_csv(path, encoding = 'unicode-escape')\n",
    "\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Goal: Cluster yelp text to see if we can find which star rating they belong to.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = yelp.text\n",
    "y = yelp.stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#see RandomSearchCV for tfidf parameters below. This is the best combination!\n",
    "\n",
    "{'tfidfvectorizer__analyzer': <function __main__.word_tokenize_lemma>,\n",
    " 'tfidfvectorizer__binary': True,\n",
    " 'tfidfvectorizer__lowercase': True,\n",
    " 'tfidfvectorizer__max_features': 1000,\n",
    " 'tfidfvectorizer__ngram_range': (2, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Intialize tfidf with the above parameters\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=1000, ngram_range = (2, 2),\n",
    "                        analyzer=word_tokenize_lemma, binary = True, lowercase = True)\n",
    "\n",
    "#Fit and transform the text using the tfidf vectorizer\n",
    "text = articles.text\n",
    "dtm = tfidf.fit_transform(text)\n",
    "\n",
    "#Assign tokens to features\n",
    "features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a dataframe of features and their idf scores\n",
    "idfscores = pd.DataFrame()\n",
    "idfscores[\"tokens\"] = features\n",
    "idfscores[\"scores\"] = tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>talking</td>\n",
       "      <td>3.478584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>described</td>\n",
       "      <td>3.478584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>scenario</td>\n",
       "      <td>3.478584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>low</td>\n",
       "      <td>3.478584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>healthcare</td>\n",
       "      <td>3.478584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>allowing</td>\n",
       "      <td>3.478584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>told</td>\n",
       "      <td>3.478584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>announced</td>\n",
       "      <td>3.478584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>unit</td>\n",
       "      <td>3.478584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>extract</td>\n",
       "      <td>3.478584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Just</td>\n",
       "      <td>3.470216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>sound</td>\n",
       "      <td>3.470216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>partner</td>\n",
       "      <td>3.470216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>client</td>\n",
       "      <td>3.470216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>3.470216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>considered</td>\n",
       "      <td>3.470216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>act</td>\n",
       "      <td>3.461917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>transformation</td>\n",
       "      <td>3.461917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>wide</td>\n",
       "      <td>3.461917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>3.461917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tokens    scores\n",
       "864         talking  3.478584\n",
       "307       described  3.478584\n",
       "776        scenario  3.478584\n",
       "571             low  3.478584\n",
       "463      healthcare  3.478584\n",
       "140        allowing  3.478584\n",
       "902            told  3.478584\n",
       "156       announced  3.478584\n",
       "930            unit  3.478584\n",
       "387         extract  3.478584\n",
       "61             Just  3.470216\n",
       "823           sound  3.470216\n",
       "665         partner  3.470216\n",
       "247          client  3.470216\n",
       "5                15  3.470216\n",
       "273      considered  3.470216\n",
       "117             act  3.461917\n",
       "916  transformation  3.461917\n",
       "975            wide  3.461917\n",
       "4                12  3.461917"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfscores.sort_values(by = \"scores\", ascending = False).head(20)\n",
    "#the words changed here. Previously, it was \"bagel\", \"donuts\", \"nail\" etc. More nouncs, now they're more like verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>to</td>\n",
       "      <td>1.013480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>the</td>\n",
       "      <td>1.017775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>of</td>\n",
       "      <td>1.020649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>a</td>\n",
       "      <td>1.024975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>and</td>\n",
       "      <td>1.025697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>in</td>\n",
       "      <td>1.037334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>is</td>\n",
       "      <td>1.050589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>for</td>\n",
       "      <td>1.060272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>that</td>\n",
       "      <td>1.070051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>with</td>\n",
       "      <td>1.074596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tokens    scores\n",
       "899     to  1.013480\n",
       "879    the  1.017775\n",
       "632     of  1.020649\n",
       "105      a  1.024975\n",
       "155    and  1.025697\n",
       "493     in  1.037334\n",
       "521     is  1.050589\n",
       "414    for  1.060272\n",
       "878   that  1.070051\n",
       "977   with  1.074596"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfscores.sort_values(by=\"scores\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    #Randomly choose index value\n",
    "    index = np.random.choice(articles.index, 1)[0]\n",
    "    article = text.iloc[index]\n",
    "    # create a dictionary of words and their TF-IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(article).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[index, features.index(word)]\n",
    "            \n",
    "   # print words with the top 5 TF-IDF scores\n",
    "    print 'TOP 10 SCORING WORDS BY REVIEW:'\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for word, score in top_scores:\n",
    "        print word   \n",
    "        \n",
    "    #Print title of article\n",
    "    print \"\\nSTAR RATING: \", yelp.stars[index]\n",
    "    \n",
    "    #Print the text of article\n",
    "#     print article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 SCORING WORDS BY REVIEW:\n",
      "pie\n",
      "neighborhood\n",
      "finish\n",
      "the\n",
      "pork\n",
      "spot\n",
      "both\n",
      "perfect\n",
      "meal\n",
      "make\n",
      "\n",
      "STAR RATING:  5\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 SCORING WORDS BY REVIEW:\n",
      "mom\n",
      "beef\n",
      "without\n",
      "and\n",
      "was\n",
      "we\n",
      "were\n",
      "over\n",
      "i\n",
      "for\n",
      "\n",
      "STAR RATING:  4\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 SCORING WORDS BY REVIEW:\n",
      "shrimp\n",
      "the\n",
      "fine\n",
      "fish\n",
      "i\n",
      "they\n",
      "and\n",
      "review\n",
      "lot\n",
      "after\n",
      "\n",
      "STAR RATING:  3\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 SCORING WORDS BY REVIEW:\n",
      "care\n",
      "she\n",
      "how\n",
      "i\n",
      "young\n",
      "they\n",
      "drop\n",
      "would\n",
      "about\n",
      "15\n",
      "\n",
      "STAR RATING:  2\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 SCORING WORDS BY REVIEW:\n",
      "the\n",
      "waiter\n",
      "we\n",
      "dessert\n",
      "our\n",
      "explain\n",
      "unless\n",
      "meal\n",
      "to\n",
      "a\n",
      "\n",
      "STAR RATING:  1\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = cosine_similarity(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265631</td>\n",
       "      <td>0.140160</td>\n",
       "      <td>0.150543</td>\n",
       "      <td>0.157744</td>\n",
       "      <td>0.310240</td>\n",
       "      <td>0.366842</td>\n",
       "      <td>0.186357</td>\n",
       "      <td>0.209724</td>\n",
       "      <td>0.133153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185904</td>\n",
       "      <td>0.206033</td>\n",
       "      <td>0.299977</td>\n",
       "      <td>0.263117</td>\n",
       "      <td>0.233961</td>\n",
       "      <td>0.223305</td>\n",
       "      <td>0.273634</td>\n",
       "      <td>0.252683</td>\n",
       "      <td>0.170309</td>\n",
       "      <td>0.165041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.265631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.095235</td>\n",
       "      <td>0.178524</td>\n",
       "      <td>0.248129</td>\n",
       "      <td>0.394907</td>\n",
       "      <td>0.329775</td>\n",
       "      <td>0.193584</td>\n",
       "      <td>0.193605</td>\n",
       "      <td>0.138281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180837</td>\n",
       "      <td>0.253761</td>\n",
       "      <td>0.329982</td>\n",
       "      <td>0.246323</td>\n",
       "      <td>0.348538</td>\n",
       "      <td>0.417698</td>\n",
       "      <td>0.296040</td>\n",
       "      <td>0.274741</td>\n",
       "      <td>0.250620</td>\n",
       "      <td>0.228771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.140160</td>\n",
       "      <td>0.095235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.102721</td>\n",
       "      <td>0.067641</td>\n",
       "      <td>0.134734</td>\n",
       "      <td>0.162686</td>\n",
       "      <td>0.053588</td>\n",
       "      <td>0.026819</td>\n",
       "      <td>0.024263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089237</td>\n",
       "      <td>0.099667</td>\n",
       "      <td>0.124686</td>\n",
       "      <td>0.135001</td>\n",
       "      <td>0.090076</td>\n",
       "      <td>0.098566</td>\n",
       "      <td>0.123984</td>\n",
       "      <td>0.131283</td>\n",
       "      <td>0.061732</td>\n",
       "      <td>0.079356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.150543</td>\n",
       "      <td>0.178524</td>\n",
       "      <td>0.102721</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.124986</td>\n",
       "      <td>0.185893</td>\n",
       "      <td>0.214690</td>\n",
       "      <td>0.113088</td>\n",
       "      <td>0.084104</td>\n",
       "      <td>0.108312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163275</td>\n",
       "      <td>0.230002</td>\n",
       "      <td>0.253871</td>\n",
       "      <td>0.128041</td>\n",
       "      <td>0.249466</td>\n",
       "      <td>0.115347</td>\n",
       "      <td>0.177156</td>\n",
       "      <td>0.221494</td>\n",
       "      <td>0.160121</td>\n",
       "      <td>0.108506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.157744</td>\n",
       "      <td>0.248129</td>\n",
       "      <td>0.067641</td>\n",
       "      <td>0.124986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.190418</td>\n",
       "      <td>0.205416</td>\n",
       "      <td>0.175190</td>\n",
       "      <td>0.141164</td>\n",
       "      <td>0.132470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118275</td>\n",
       "      <td>0.154964</td>\n",
       "      <td>0.237319</td>\n",
       "      <td>0.119964</td>\n",
       "      <td>0.255874</td>\n",
       "      <td>0.131317</td>\n",
       "      <td>0.184400</td>\n",
       "      <td>0.205218</td>\n",
       "      <td>0.175987</td>\n",
       "      <td>0.096345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  1.000000  0.265631  0.140160  0.150543  0.157744  0.310240  0.366842   \n",
       "1  0.265631  1.000000  0.095235  0.178524  0.248129  0.394907  0.329775   \n",
       "2  0.140160  0.095235  1.000000  0.102721  0.067641  0.134734  0.162686   \n",
       "3  0.150543  0.178524  0.102721  1.000000  0.124986  0.185893  0.214690   \n",
       "4  0.157744  0.248129  0.067641  0.124986  1.000000  0.190418  0.205416   \n",
       "\n",
       "       7         8         9       ...         9990      9991      9992  \\\n",
       "0  0.186357  0.209724  0.133153    ...     0.185904  0.206033  0.299977   \n",
       "1  0.193584  0.193605  0.138281    ...     0.180837  0.253761  0.329982   \n",
       "2  0.053588  0.026819  0.024263    ...     0.089237  0.099667  0.124686   \n",
       "3  0.113088  0.084104  0.108312    ...     0.163275  0.230002  0.253871   \n",
       "4  0.175190  0.141164  0.132470    ...     0.118275  0.154964  0.237319   \n",
       "\n",
       "       9993      9994      9995      9996      9997      9998      9999  \n",
       "0  0.263117  0.233961  0.223305  0.273634  0.252683  0.170309  0.165041  \n",
       "1  0.246323  0.348538  0.417698  0.296040  0.274741  0.250620  0.228771  \n",
       "2  0.135001  0.090076  0.098566  0.123984  0.131283  0.061732  0.079356  \n",
       "3  0.128041  0.249466  0.115347  0.177156  0.221494  0.160121  0.108506  \n",
       "4  0.119964  0.255874  0.131317  0.184400  0.205218  0.175987  0.096345  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make it a dataframe\n",
    "dist_df = pd.DataFrame(dist)\n",
    "\n",
    "#Shape\n",
    "dist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Intialize clustering algorithm with 4 clusters\n",
    "km_yelp = KMeans(n_clusters=5)\n",
    "\n",
    "#fit it on dist array\n",
    "\n",
    "km_yelp.fit(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Intialize range of cluster values from 2 to 16\n",
    "cluster_range = range(2, 7)\n",
    "\n",
    "#Intialize list to store inertia scores\n",
    "\n",
    "i_scores = []\n",
    "\n",
    "#Iterate over cluster range, fit models and add score to s_scores\n",
    "\n",
    "for cluster in cluster_range:\n",
    "    model = KMeans(n_clusters=cluster)\n",
    "    model.fit(dist)\n",
    "    \n",
    "    score = model.inertia_\n",
    "    i_scores.append(score)\n",
    "    \n",
    "#Plot clusters versus scores\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(cluster_range, i_scores, linewidth = 6, alpha = .8, c = \"g\")\n",
    "plt.xlabel(\"Cluster Values\")\n",
    "plt.ylabel(\"Inertia Scores\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14864134505266238"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out silhouette score\n",
    "silhouette_score(dist, km_yelp.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp[\"cluster_dist\"] = km_yelp.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#Cluster 0\n",
    "for i in yelp[yelp.cluster_dist == 0].sample(n=5).stars.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "1\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#Cluster 1\n",
    "for i in yelp[yelp.cluster_dist == 1].sample(n=5).stars.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "5\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#Cluster 2\n",
    "for i in yelp[yelp.cluster_dist == 2].sample(n=5).stars.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#Cluster 3\n",
    "for i in yelp[yelp.cluster_dist == 3].sample(n=5).stars.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "1\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Cluster 4\n",
    "for i in yelp[yelp.cluster_dist == 4].sample(n=5).stars.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame called yelp_best_worst that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    0.816691\n",
      "1    0.183309\n",
      "Name: stars, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "#Null accuracy\n",
    "print y.value_counts(normalize=True)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make pipeline for countvectorizer and naive bayes model\n",
    "pipe_cv = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "#Intialize parameters for count vectorizer\n",
    "param_grid_cv = {}\n",
    "param_grid_cv[\"countvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_cv[\"countvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_cv[\"countvectorizer__lowercase\"] = [True, False]\n",
    "param_grid_cv[\"countvectorizer__binary\"] = [True, False]\n",
    "param_grid_cv[\"countvectorizer__analyzer\"] = [\"word\", word_tokenize_stem,\n",
    "                                              word_tokenize_lemma, word_tokenize_lemma_verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407.359891891\n"
     ]
    }
   ],
   "source": [
    "#Randomized grid search with n_iter = 5\n",
    "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 5,\n",
    "                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "randsearch_cv.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__analyzer': <function __main__.word_tokenize_lemma>,\n",
       " 'countvectorizer__binary': True,\n",
       " 'countvectorizer__lowercase': False,\n",
       " 'countvectorizer__max_features': 5000,\n",
       " 'countvectorizer__ngram_range': (2, 2)}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randsearch_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9287812041116006"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randsearch_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make pipeline for tfidfvectorizer and naive bayes model\n",
    "pipe_tf = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "\n",
    "#Intialize parameters for tfidf vectorizer\n",
    "param_grid_tf = {}\n",
    "param_grid_tf[\"tfidfvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_tf[\"tfidfvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_tf[\"tfidfvectorizer__lowercase\"] = [True, False]\n",
    "param_grid_tf[\"tfidfvectorizer__binary\"] = [True, False]\n",
    "param_grid_tf[\"tfidfvectorizer__analyzer\"] = [\"word\", word_tokenize_stem,\n",
    "                                              word_tokenize_lemma, word_tokenize_lemma_verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3835.05065179\n"
     ]
    }
   ],
   "source": [
    "#Randomized grid search with n_iter = 10\n",
    "randsearch_tf = RandomizedSearchCV(pipe_tf, n_iter = 10,\n",
    "                        param_distributions = param_grid_tf, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "randsearch_tf.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidfvectorizer__analyzer': <function __main__.word_tokenize_lemma>,\n",
       " 'tfidfvectorizer__binary': True,\n",
       " 'tfidfvectorizer__lowercase': True,\n",
       " 'tfidfvectorizer__max_features': 1000,\n",
       " 'tfidfvectorizer__ngram_range': (2, 2)}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randsearch_tf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8607440039158101"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randsearch_tf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
